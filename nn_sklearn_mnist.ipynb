{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision WS2020/2021 - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a simple neural network framework using Python and NumPy without using more complex frameworks such as Tensorflow, Scikit or Pytorch.\n",
    "\n",
    "It should be possible to define different neural network parameters, such as: layer-type (input, hidden, output, dropout, activation), number of layers, neurons per layer, activation function, learning rate etc.\n",
    "\n",
    "In the end, final framework should be applied on a computer vision dataset. For this could be used MNIST dataset to recognize handwritten digits. Following, the result and performance can be compared with the standard-sklearn implementation (MLPClassifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of this exercise contains following steps:\n",
    "- Own neural network implementation\n",
    "- Importing and preparting MNIST dataset to be ready for training \n",
    "- Training the dataset on own neural network model\n",
    "- Training the dataset on Sklearn model\n",
    "- Comparing performance between these two networks/models (similar properties will be used for training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network is implemented in a separated file named 'neural_network.py'. From there will be imported all classes and methods used in this exercise/notebook for custom-network training and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 What is implemented:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Different layers to be added to the model (Input, FC/Dense, Activation Layer)\n",
    "- Different activation functions (relu, sigmoid, softmax)\n",
    "- Loss functions (MSE - Mean Squared Error, Categorical Cross Entropy)\n",
    "- Accuracy (Categorical accuracy)\n",
    "- Optimizer (Adam)\n",
    "- Neural network model class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Import MNIST Dataset & Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first import all required libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeding random number generators to obtain reproducible results\n",
    "seed_value = 0\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      7    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv(\"./Data/mnist.csv\", delimiter=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input features: 784\n",
      "\n",
      "Uniqe classes: [7 2 1 0 4 9 5 6 3 8]\n",
      "Number of output classes: 10\n"
     ]
    }
   ],
   "source": [
    "# some more data on dataset\n",
    "print(\"Number of input features:\", len(df.columns[1:]))\n",
    "print(\"\\nUniqe classes:\", df['label'].unique())\n",
    "print(\"Number of output classes:\", len(df['label'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train data:      7000\n",
      "Number of test data:       3000\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing, extract labels and features\n",
    "labels = np.array(df.iloc[:,0])\n",
    "features = np.array(df.iloc[:,1:])\n",
    "\n",
    "# split training, validation and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, train_size=0.7, random_state=seed_value)\n",
    "\n",
    "print(\"Number of train data:     \", len(X_train))\n",
    "print(\"Number of test data:      \", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise data\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Neural Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import implemenation from file\n",
    "import neural_network as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200, acc: 0.092, data_loss: 2.303, lr:0.003\n",
      "Epoch: 2/200, acc: 0.197, data_loss: 2.300, lr:0.0029970029970029974\n",
      "Epoch: 3/200, acc: 0.205, data_loss: 2.293, lr:0.002994011976047904\n",
      "Epoch: 4/200, acc: 0.280, data_loss: 2.279, lr:0.002991026919242274\n",
      "Epoch: 5/200, acc: 0.354, data_loss: 2.253, lr:0.00298804780876494\n",
      "Epoch: 6/200, acc: 0.409, data_loss: 2.212, lr:0.002985074626865672\n",
      "Epoch: 7/200, acc: 0.443, data_loss: 2.152, lr:0.0029821073558648115\n",
      "Epoch: 8/200, acc: 0.462, data_loss: 2.072, lr:0.00297914597815293\n",
      "Epoch: 9/200, acc: 0.473, data_loss: 1.969, lr:0.0029761904761904765\n",
      "Epoch: 10/200, acc: 0.484, data_loss: 1.846, lr:0.0029732408325074335\n",
      "Epoch: 11/200, acc: 0.504, data_loss: 1.709, lr:0.0029702970297029703\n",
      "Epoch: 12/200, acc: 0.530, data_loss: 1.565, lr:0.0029673590504451044\n",
      "Epoch: 13/200, acc: 0.577, data_loss: 1.421, lr:0.0029644268774703555\n",
      "Epoch: 14/200, acc: 0.639, data_loss: 1.287, lr:0.002961500493583416\n",
      "Epoch: 15/200, acc: 0.688, data_loss: 1.166, lr:0.002958579881656805\n",
      "Epoch: 16/200, acc: 0.718, data_loss: 1.063, lr:0.0029556650246305425\n",
      "Epoch: 17/200, acc: 0.731, data_loss: 0.981, lr:0.002952755905511811\n",
      "Epoch: 18/200, acc: 0.738, data_loss: 0.914, lr:0.0029498525073746317\n",
      "Epoch: 19/200, acc: 0.744, data_loss: 0.859, lr:0.0029469548133595285\n",
      "Epoch: 20/200, acc: 0.752, data_loss: 0.815, lr:0.0029440628066732095\n",
      "Epoch: 21/200, acc: 0.759, data_loss: 0.778, lr:0.0029411764705882353\n",
      "Epoch: 22/200, acc: 0.768, data_loss: 0.744, lr:0.0029382957884427035\n",
      "Epoch: 23/200, acc: 0.778, data_loss: 0.714, lr:0.0029354207436399216\n",
      "Epoch: 24/200, acc: 0.785, data_loss: 0.686, lr:0.0029325513196480943\n",
      "Epoch: 25/200, acc: 0.794, data_loss: 0.659, lr:0.0029296875\n",
      "Epoch: 26/200, acc: 0.802, data_loss: 0.632, lr:0.0029268292682926834\n",
      "Epoch: 27/200, acc: 0.811, data_loss: 0.608, lr:0.0029239766081871343\n",
      "Epoch: 28/200, acc: 0.817, data_loss: 0.586, lr:0.0029211295034079847\n",
      "Epoch: 29/200, acc: 0.825, data_loss: 0.563, lr:0.0029182879377431907\n",
      "Epoch: 30/200, acc: 0.832, data_loss: 0.543, lr:0.002915451895043732\n",
      "Epoch: 31/200, acc: 0.837, data_loss: 0.523, lr:0.002912621359223301\n",
      "Epoch: 32/200, acc: 0.841, data_loss: 0.505, lr:0.0029097963142580025\n",
      "Epoch: 33/200, acc: 0.845, data_loss: 0.488, lr:0.0029069767441860465\n",
      "Epoch: 34/200, acc: 0.849, data_loss: 0.471, lr:0.0029041626331074545\n",
      "Epoch: 35/200, acc: 0.855, data_loss: 0.456, lr:0.002901353965183752\n",
      "Epoch: 36/200, acc: 0.859, data_loss: 0.442, lr:0.0028985507246376816\n",
      "Epoch: 37/200, acc: 0.866, data_loss: 0.428, lr:0.0028957528957528956\n",
      "Epoch: 38/200, acc: 0.870, data_loss: 0.415, lr:0.0028929604628736743\n",
      "Epoch: 39/200, acc: 0.874, data_loss: 0.403, lr:0.002890173410404624\n",
      "Epoch: 40/200, acc: 0.878, data_loss: 0.393, lr:0.002887391722810395\n",
      "Epoch: 41/200, acc: 0.878, data_loss: 0.383, lr:0.0028846153846153843\n",
      "Epoch: 42/200, acc: 0.883, data_loss: 0.373, lr:0.0028818443804034585\n",
      "Epoch: 43/200, acc: 0.884, data_loss: 0.364, lr:0.0028790786948176585\n",
      "Epoch: 44/200, acc: 0.890, data_loss: 0.355, lr:0.0028763183125599234\n",
      "Epoch: 45/200, acc: 0.892, data_loss: 0.347, lr:0.0028735632183908046\n",
      "Epoch: 46/200, acc: 0.893, data_loss: 0.340, lr:0.002870813397129187\n",
      "Epoch: 47/200, acc: 0.896, data_loss: 0.333, lr:0.0028680688336520078\n",
      "Epoch: 48/200, acc: 0.898, data_loss: 0.327, lr:0.0028653295128939827\n",
      "Epoch: 49/200, acc: 0.901, data_loss: 0.320, lr:0.0028625954198473282\n",
      "Epoch: 50/200, acc: 0.903, data_loss: 0.314, lr:0.002859866539561487\n",
      "Epoch: 51/200, acc: 0.905, data_loss: 0.308, lr:0.002857142857142857\n",
      "Epoch: 52/200, acc: 0.907, data_loss: 0.302, lr:0.00285442435775452\n",
      "Epoch: 53/200, acc: 0.908, data_loss: 0.296, lr:0.0028517110266159692\n",
      "Epoch: 54/200, acc: 0.909, data_loss: 0.290, lr:0.002849002849002849\n",
      "Epoch: 55/200, acc: 0.913, data_loss: 0.284, lr:0.0028462998102466793\n",
      "Epoch: 56/200, acc: 0.916, data_loss: 0.278, lr:0.0028436018957345975\n",
      "Epoch: 57/200, acc: 0.917, data_loss: 0.273, lr:0.002840909090909091\n",
      "Epoch: 58/200, acc: 0.919, data_loss: 0.267, lr:0.0028382213812677393\n",
      "Epoch: 59/200, acc: 0.921, data_loss: 0.262, lr:0.0028355387523629487\n",
      "Epoch: 60/200, acc: 0.923, data_loss: 0.257, lr:0.0028328611898017\n",
      "Epoch: 61/200, acc: 0.924, data_loss: 0.252, lr:0.002830188679245283\n",
      "Epoch: 62/200, acc: 0.925, data_loss: 0.247, lr:0.002827521206409048\n",
      "Epoch: 63/200, acc: 0.928, data_loss: 0.242, lr:0.002824858757062147\n",
      "Epoch: 64/200, acc: 0.929, data_loss: 0.237, lr:0.0028222013170272815\n",
      "Epoch: 65/200, acc: 0.930, data_loss: 0.232, lr:0.0028195488721804514\n",
      "Epoch: 66/200, acc: 0.932, data_loss: 0.228, lr:0.0028169014084507044\n",
      "Epoch: 67/200, acc: 0.933, data_loss: 0.223, lr:0.0028142589118198874\n",
      "Epoch: 68/200, acc: 0.934, data_loss: 0.219, lr:0.0028116213683223993\n",
      "Epoch: 69/200, acc: 0.935, data_loss: 0.215, lr:0.0028089887640449437\n",
      "Epoch: 70/200, acc: 0.937, data_loss: 0.210, lr:0.0028063610851262865\n",
      "Epoch: 71/200, acc: 0.939, data_loss: 0.206, lr:0.0028037383177570096\n",
      "Epoch: 72/200, acc: 0.940, data_loss: 0.202, lr:0.0028011204481792717\n",
      "Epoch: 73/200, acc: 0.941, data_loss: 0.198, lr:0.002798507462686567\n",
      "Epoch: 74/200, acc: 0.942, data_loss: 0.194, lr:0.0027958993476234857\n",
      "Epoch: 75/200, acc: 0.944, data_loss: 0.190, lr:0.0027932960893854745\n",
      "Epoch: 76/200, acc: 0.945, data_loss: 0.186, lr:0.0027906976744186047\n",
      "Epoch: 77/200, acc: 0.947, data_loss: 0.182, lr:0.0027881040892193307\n",
      "Epoch: 78/200, acc: 0.949, data_loss: 0.178, lr:0.002785515320334262\n",
      "Epoch: 79/200, acc: 0.950, data_loss: 0.175, lr:0.0027829313543599257\n",
      "Epoch: 80/200, acc: 0.952, data_loss: 0.171, lr:0.0027803521779425394\n",
      "Epoch: 81/200, acc: 0.953, data_loss: 0.167, lr:0.0027777777777777775\n",
      "Epoch: 82/200, acc: 0.954, data_loss: 0.164, lr:0.0027752081406105457\n",
      "Epoch: 83/200, acc: 0.955, data_loss: 0.160, lr:0.0027726432532347504\n",
      "Epoch: 84/200, acc: 0.956, data_loss: 0.157, lr:0.0027700831024930752\n",
      "Epoch: 85/200, acc: 0.957, data_loss: 0.153, lr:0.0027675276752767526\n",
      "Epoch: 86/200, acc: 0.958, data_loss: 0.150, lr:0.0027649769585253456\n",
      "Epoch: 87/200, acc: 0.959, data_loss: 0.147, lr:0.0027624309392265192\n",
      "Epoch: 88/200, acc: 0.960, data_loss: 0.143, lr:0.0027598896044158236\n",
      "Epoch: 89/200, acc: 0.961, data_loss: 0.140, lr:0.0027573529411764703\n",
      "Epoch: 90/200, acc: 0.962, data_loss: 0.137, lr:0.0027548209366391185\n",
      "Epoch: 91/200, acc: 0.963, data_loss: 0.134, lr:0.002752293577981651\n",
      "Epoch: 92/200, acc: 0.963, data_loss: 0.131, lr:0.0027497708524289646\n",
      "Epoch: 93/200, acc: 0.964, data_loss: 0.128, lr:0.002747252747252747\n",
      "Epoch: 94/200, acc: 0.965, data_loss: 0.125, lr:0.002744739249771272\n",
      "Epoch: 95/200, acc: 0.966, data_loss: 0.122, lr:0.002742230347349177\n",
      "Epoch: 96/200, acc: 0.967, data_loss: 0.120, lr:0.0027397260273972603\n",
      "Epoch: 97/200, acc: 0.968, data_loss: 0.117, lr:0.0027372262773722625\n",
      "Epoch: 98/200, acc: 0.970, data_loss: 0.114, lr:0.0027347310847766638\n",
      "Epoch: 99/200, acc: 0.970, data_loss: 0.112, lr:0.00273224043715847\n",
      "Epoch: 100/200, acc: 0.970, data_loss: 0.109, lr:0.00272975432211101\n",
      "Epoch: 101/200, acc: 0.971, data_loss: 0.107, lr:0.002727272727272727\n",
      "Epoch: 102/200, acc: 0.972, data_loss: 0.104, lr:0.0027247956403269754\n",
      "Epoch: 103/200, acc: 0.974, data_loss: 0.102, lr:0.0027223230490018148\n",
      "Epoch: 104/200, acc: 0.974, data_loss: 0.099, lr:0.0027198549410698096\n",
      "Epoch: 105/200, acc: 0.975, data_loss: 0.097, lr:0.002717391304347826\n",
      "Epoch: 106/200, acc: 0.976, data_loss: 0.095, lr:0.0027149321266968325\n",
      "Epoch: 107/200, acc: 0.976, data_loss: 0.093, lr:0.0027124773960217\n",
      "Epoch: 108/200, acc: 0.977, data_loss: 0.090, lr:0.002710027100271003\n",
      "Epoch: 109/200, acc: 0.978, data_loss: 0.088, lr:0.002707581227436823\n",
      "Epoch: 110/200, acc: 0.979, data_loss: 0.086, lr:0.002705139765554554\n",
      "Epoch: 111/200, acc: 0.980, data_loss: 0.084, lr:0.0027027027027027024\n",
      "Epoch: 112/200, acc: 0.981, data_loss: 0.082, lr:0.0027002700270027003\n",
      "Epoch: 113/200, acc: 0.982, data_loss: 0.080, lr:0.002697841726618705\n",
      "Epoch: 114/200, acc: 0.983, data_loss: 0.078, lr:0.0026954177897574125\n",
      "Epoch: 115/200, acc: 0.984, data_loss: 0.076, lr:0.0026929982046678632\n",
      "Epoch: 116/200, acc: 0.984, data_loss: 0.074, lr:0.0026905829596412557\n",
      "Epoch: 117/200, acc: 0.985, data_loss: 0.072, lr:0.0026881720430107525\n",
      "Epoch: 118/200, acc: 0.986, data_loss: 0.071, lr:0.0026857654431512983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 119/200, acc: 0.986, data_loss: 0.069, lr:0.0026833631484794273\n",
      "Epoch: 120/200, acc: 0.987, data_loss: 0.067, lr:0.002680965147453083\n",
      "Epoch: 121/200, acc: 0.987, data_loss: 0.065, lr:0.0026785714285714286\n",
      "Epoch: 122/200, acc: 0.987, data_loss: 0.064, lr:0.0026761819803746657\n",
      "Epoch: 123/200, acc: 0.988, data_loss: 0.062, lr:0.0026737967914438505\n",
      "Epoch: 124/200, acc: 0.988, data_loss: 0.061, lr:0.0026714158504007124\n",
      "Epoch: 125/200, acc: 0.988, data_loss: 0.059, lr:0.002669039145907473\n",
      "Epoch: 126/200, acc: 0.989, data_loss: 0.058, lr:0.0026666666666666666\n",
      "Epoch: 127/200, acc: 0.989, data_loss: 0.056, lr:0.0026642984014209597\n",
      "Epoch: 128/200, acc: 0.990, data_loss: 0.055, lr:0.0026619343389529724\n",
      "Epoch: 129/200, acc: 0.990, data_loss: 0.053, lr:0.0026595744680851063\n",
      "Epoch: 130/200, acc: 0.991, data_loss: 0.052, lr:0.002657218777679362\n",
      "Epoch: 131/200, acc: 0.991, data_loss: 0.051, lr:0.0026548672566371685\n",
      "Epoch: 132/200, acc: 0.992, data_loss: 0.049, lr:0.0026525198938992045\n",
      "Epoch: 133/200, acc: 0.992, data_loss: 0.048, lr:0.0026501766784452294\n",
      "Epoch: 134/200, acc: 0.992, data_loss: 0.047, lr:0.0026478375992939097\n",
      "Epoch: 135/200, acc: 0.993, data_loss: 0.046, lr:0.002645502645502646\n",
      "Epoch: 136/200, acc: 0.993, data_loss: 0.044, lr:0.0026431718061674008\n",
      "Epoch: 137/200, acc: 0.993, data_loss: 0.043, lr:0.002640845070422535\n",
      "Epoch: 138/200, acc: 0.994, data_loss: 0.042, lr:0.002638522427440633\n",
      "Epoch: 139/200, acc: 0.994, data_loss: 0.041, lr:0.002636203866432338\n",
      "Epoch: 140/200, acc: 0.994, data_loss: 0.040, lr:0.0026338893766461808\n",
      "Epoch: 141/200, acc: 0.994, data_loss: 0.039, lr:0.002631578947368421\n",
      "Epoch: 142/200, acc: 0.994, data_loss: 0.038, lr:0.0026292725679228747\n",
      "Epoch: 143/200, acc: 0.994, data_loss: 0.037, lr:0.002626970227670753\n",
      "Epoch: 144/200, acc: 0.995, data_loss: 0.036, lr:0.0026246719160104987\n",
      "Epoch: 145/200, acc: 0.995, data_loss: 0.035, lr:0.002622377622377622\n",
      "Epoch: 146/200, acc: 0.995, data_loss: 0.034, lr:0.002620087336244542\n",
      "Epoch: 147/200, acc: 0.995, data_loss: 0.034, lr:0.002617801047120419\n",
      "Epoch: 148/200, acc: 0.995, data_loss: 0.033, lr:0.0026155187445510023\n",
      "Epoch: 149/200, acc: 0.995, data_loss: 0.032, lr:0.002613240418118467\n",
      "Epoch: 150/200, acc: 0.995, data_loss: 0.031, lr:0.0026109660574412533\n",
      "Epoch: 151/200, acc: 0.996, data_loss: 0.030, lr:0.0026086956521739132\n",
      "Epoch: 152/200, acc: 0.996, data_loss: 0.029, lr:0.0026064291920069507\n",
      "Epoch: 153/200, acc: 0.996, data_loss: 0.029, lr:0.002604166666666667\n",
      "Epoch: 154/200, acc: 0.996, data_loss: 0.028, lr:0.0026019080659150044\n",
      "Epoch: 155/200, acc: 0.996, data_loss: 0.027, lr:0.0025996533795493936\n",
      "Epoch: 156/200, acc: 0.996, data_loss: 0.027, lr:0.0025974025974025974\n",
      "Epoch: 157/200, acc: 0.996, data_loss: 0.026, lr:0.002595155709342561\n",
      "Epoch: 158/200, acc: 0.997, data_loss: 0.025, lr:0.0025929127052722557\n",
      "Epoch: 159/200, acc: 0.997, data_loss: 0.025, lr:0.002590673575129534\n",
      "Epoch: 160/200, acc: 0.997, data_loss: 0.024, lr:0.0025884383088869713\n",
      "Epoch: 161/200, acc: 0.997, data_loss: 0.023, lr:0.0025862068965517245\n",
      "Epoch: 162/200, acc: 0.997, data_loss: 0.023, lr:0.0025839793281653744\n",
      "Epoch: 163/200, acc: 0.997, data_loss: 0.022, lr:0.0025817555938037868\n",
      "Epoch: 164/200, acc: 0.997, data_loss: 0.022, lr:0.002579535683576956\n",
      "Epoch: 165/200, acc: 0.998, data_loss: 0.021, lr:0.0025773195876288664\n",
      "Epoch: 166/200, acc: 0.998, data_loss: 0.021, lr:0.002575107296137339\n",
      "Epoch: 167/200, acc: 0.998, data_loss: 0.020, lr:0.002572898799313894\n",
      "Epoch: 168/200, acc: 0.998, data_loss: 0.020, lr:0.002570694087403599\n",
      "Epoch: 169/200, acc: 0.998, data_loss: 0.019, lr:0.002568493150684932\n",
      "Epoch: 170/200, acc: 0.998, data_loss: 0.019, lr:0.0025662959794696323\n",
      "Epoch: 171/200, acc: 0.998, data_loss: 0.018, lr:0.0025641025641025645\n",
      "Epoch: 172/200, acc: 0.999, data_loss: 0.018, lr:0.0025619128949615714\n",
      "Epoch: 173/200, acc: 0.999, data_loss: 0.017, lr:0.0025597269624573382\n",
      "Epoch: 174/200, acc: 0.999, data_loss: 0.017, lr:0.002557544757033248\n",
      "Epoch: 175/200, acc: 0.999, data_loss: 0.016, lr:0.0025553662691652473\n",
      "Epoch: 176/200, acc: 0.999, data_loss: 0.016, lr:0.002553191489361702\n",
      "Epoch: 177/200, acc: 0.999, data_loss: 0.016, lr:0.0025510204081632655\n",
      "Epoch: 178/200, acc: 0.999, data_loss: 0.015, lr:0.002548853016142736\n",
      "Epoch: 179/200, acc: 0.999, data_loss: 0.015, lr:0.0025466893039049238\n",
      "Epoch: 180/200, acc: 0.999, data_loss: 0.015, lr:0.0025445292620865138\n",
      "Epoch: 181/200, acc: 0.999, data_loss: 0.014, lr:0.0025423728813559325\n",
      "Epoch: 182/200, acc: 0.999, data_loss: 0.014, lr:0.0025402201524132094\n",
      "Epoch: 183/200, acc: 0.999, data_loss: 0.014, lr:0.002538071065989848\n",
      "Epoch: 184/200, acc: 0.999, data_loss: 0.013, lr:0.00253592561284869\n",
      "Epoch: 185/200, acc: 0.999, data_loss: 0.013, lr:0.002533783783783784\n",
      "Epoch: 186/200, acc: 0.999, data_loss: 0.013, lr:0.0025316455696202528\n",
      "Epoch: 187/200, acc: 0.999, data_loss: 0.012, lr:0.0025295109612141656\n",
      "Epoch: 188/200, acc: 1.000, data_loss: 0.012, lr:0.002527379949452401\n",
      "Epoch: 189/200, acc: 1.000, data_loss: 0.012, lr:0.0025252525252525255\n",
      "Epoch: 190/200, acc: 1.000, data_loss: 0.012, lr:0.002523128679562658\n",
      "Epoch: 191/200, acc: 1.000, data_loss: 0.011, lr:0.0025210084033613447\n",
      "Epoch: 192/200, acc: 1.000, data_loss: 0.011, lr:0.0025188916876574307\n",
      "Epoch: 193/200, acc: 1.000, data_loss: 0.011, lr:0.002516778523489933\n",
      "Epoch: 194/200, acc: 1.000, data_loss: 0.011, lr:0.002514668901927913\n",
      "Epoch: 195/200, acc: 1.000, data_loss: 0.010, lr:0.002512562814070352\n",
      "Epoch: 196/200, acc: 1.000, data_loss: 0.010, lr:0.002510460251046025\n",
      "Epoch: 197/200, acc: 1.000, data_loss: 0.010, lr:0.002508361204013378\n",
      "Epoch: 198/200, acc: 1.000, data_loss: 0.010, lr:0.002506265664160401\n",
      "Epoch: 199/200, acc: 1.000, data_loss: 0.010, lr:0.0025041736227045075\n",
      "Epoch: 200/200, acc: 1.000, data_loss: 0.009, lr:0.0025020850708924102\n"
     ]
    }
   ],
   "source": [
    "# create model and add layers\n",
    "model = nn.NetModel()\n",
    "\n",
    "# NOTE: number if outputs must match number of inputs for the next layer\n",
    "model.add(nn.DenseLayer(784, 128))\n",
    "model.add(nn.ActivationReLU())\n",
    "model.add(nn.DenseLayer(128, 64))\n",
    "model.add(nn.ActivationReLU())\n",
    "model.add(nn.DenseLayer(64, 10))\n",
    "model.add(nn.ActivationSoftmax())\n",
    "\n",
    "# set loss function, accuracy and optimizer\n",
    "model.set(\n",
    "    loss = nn.CategoricalCrossEntropyLoss(),\n",
    "    accuracy = nn.CategoricalAccuracy(),\n",
    "    optimizer = nn.AdamOptimizer(learning_rate=0.003, decay=0.001, epsilon=1e-8)\n",
    ")\n",
    "\n",
    "# compile model\n",
    "model.compile()\n",
    "\n",
    "# train model on train data\n",
    "model.train(X_train, y_train, epochs=200, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.934\n"
     ]
    }
   ],
   "source": [
    "# Get accuracy on TEST set\n",
    "acc = model.evaluate(X_test, y_test)\n",
    "print('Accuracy: {0:.3f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 MLPClassifier Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required classes\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model based on same params as for own neural network\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(128,64,),\n",
    "                    activation='relu',\n",
    "                    solver='adam',\n",
    "                    random_state=0,\n",
    "                    shuffle=False,\n",
    "                    learning_rate_init=0.003,\n",
    "                    epsilon=1e-8,\n",
    "                    max_iter=100,\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.77943445\n",
      "Iteration 2, loss = 0.27027456\n",
      "Iteration 3, loss = 0.18125347\n",
      "Iteration 4, loss = 0.13651486\n",
      "Iteration 5, loss = 0.10455411\n",
      "Iteration 6, loss = 0.07953390\n",
      "Iteration 7, loss = 0.06521334\n",
      "Iteration 8, loss = 0.06265738\n",
      "Iteration 9, loss = 0.05171165\n",
      "Iteration 10, loss = 0.04000528\n",
      "Iteration 11, loss = 0.03535576\n",
      "Iteration 12, loss = 0.02803924\n",
      "Iteration 13, loss = 0.01832743\n",
      "Iteration 14, loss = 0.01497833\n",
      "Iteration 15, loss = 0.01492761\n",
      "Iteration 16, loss = 0.01868665\n",
      "Iteration 17, loss = 0.01594991\n",
      "Iteration 18, loss = 0.01015353\n",
      "Iteration 19, loss = 0.00391874\n",
      "Iteration 20, loss = 0.00218708\n",
      "Iteration 21, loss = 0.00174251\n",
      "Iteration 22, loss = 0.00150229\n",
      "Iteration 23, loss = 0.00134198\n",
      "Iteration 24, loss = 0.00124496\n",
      "Iteration 25, loss = 0.00117862\n",
      "Iteration 26, loss = 0.00109459\n",
      "Iteration 27, loss = 0.00100325\n",
      "Iteration 28, loss = 0.00086755\n",
      "Iteration 29, loss = 0.00077154\n",
      "Iteration 30, loss = 0.00072303\n",
      "Iteration 31, loss = 0.00068990\n",
      "Iteration 32, loss = 0.00066138\n",
      "Iteration 33, loss = 0.00063579\n",
      "Iteration 34, loss = 0.00061288\n",
      "Iteration 35, loss = 0.00059183\n",
      "Iteration 36, loss = 0.00057255\n",
      "Iteration 37, loss = 0.00055488\n",
      "Iteration 38, loss = 0.00053852\n",
      "Iteration 39, loss = 0.00052327\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(128, 64), learning_rate_init=0.003,\n",
       "              max_iter=100, random_state=0, shuffle=False, verbose=True)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model on training data\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on test data:  0.957\n"
     ]
    }
   ],
   "source": [
    "# print final reuslt\n",
    "y_pred_test = mlp.predict(X_test)\n",
    "print(\"Score on test data: \", round(accuracy_score(y_test, y_pred_test), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Further Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DropoutLayer\n",
    "- L1, L2 regularization on weights during backpropagation\n",
    "- LinearActivation\n",
    "- Early Stopping mechanism\n",
    "- MeanAbosluteError, BinaryCrossEntropy Loss\n",
    "- Regression Accuracy\n",
    "- SGD, Adagrad, RMSprop Optimizer\n",
    "- Add features to model to save/load model & parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://nnfs.io/\n",
    "- https://www.youtube.com/watch?v=Wo5dMEP_BbI&list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
